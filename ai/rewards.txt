# Advanced Reward/Penalty System for Trading AI

def calculate_advanced_reward(self, action, previous_value, current_price, trade_executed, sl_triggered, tp_triggered):
    """
    Comprehensive reward function with multiple behavioral incentives
    """
    reward = 0.0
    
    # 1. PORTFOLIO PERFORMANCE REWARDS
    if previous_value > 0:
        portfolio_change = (self.total_value - previous_value) / previous_value
        reward += portfolio_change * self.portfolio_change_multiplier
    
    # 2. RISK MANAGEMENT REWARDS/PENALTIES
    
    # Drawdown penalty (encourage risk management)
    current_drawdown = (self.max_portfolio_value - self.total_value) / self.max_portfolio_value
    if current_drawdown > 0:
        reward -= current_drawdown * self.drawdown_penalty_multiplier
    
    # Volatility penalty (discourage excessive risk)
    if len(self.portfolio_history) > 10:
        recent_volatility = np.std(self.portfolio_history[-10:]) / np.mean(self.portfolio_history[-10:])
        if recent_volatility > self.max_acceptable_volatility:
            reward -= (recent_volatility - self.max_acceptable_volatility) * self.volatility_penalty
    
    # 3. POSITION SIZING REWARDS
    
    # Reward proper position sizing (not going all-in)
    position_ratio = (self.position * current_price) / self.total_value if self.position > 0 else 0
    if 0.1 <= position_ratio <= 0.8:  # Reward reasonable position sizes
        reward += self.position_sizing_reward
    elif position_ratio > 0.9:  # Penalty for over-leverage
        reward -= self.over_leverage_penalty
    
    # 4. TIMING-BASED REWARDS
    
    # Trend following reward
    price_trend = self.calculate_price_trend(window=20)
    if trade_executed:
        if action == 1 and price_trend > 0:  # Buy in uptrend
            reward += self.trend_following_reward
        elif action == 2 and price_trend < 0:  # Sell in downtrend
            reward += self.trend_following_reward
        elif action == 1 and price_trend < -0.02:  # Buy in strong downtrend
            reward -= self.counter_trend_penalty
        elif action == 2 and price_trend > 0.02:  # Sell in strong uptrend
            reward -= self.counter_trend_penalty
    
    # 5. TECHNICAL INDICATOR REWARDS
    
    # RSI-based rewards (refined)
    rsi = self.get_current_rsi()
    if not np.isnan(rsi):
        if action == 1:  # Buy action
            if rsi < 30:  # Oversold - good time to buy
                reward += self.rsi_optimal_reward
            elif rsi > 70:  # Overbought - bad time to buy
                reward -= self.rsi_suboptimal_penalty
            elif 40 <= rsi <= 60:  # Neutral zone
                reward += self.rsi_neutral_reward * 0.5
        elif action == 2:  # Sell action
            if rsi > 70:  # Overbought - good time to sell
                reward += self.rsi_optimal_reward
            elif rsi < 30:  # Oversold - bad time to sell
                reward -= self.rsi_suboptimal_penalty
    
    # MACD momentum reward
    macd_signal = self.get_macd_signal()
    if trade_executed and macd_signal is not None:
        if (action == 1 and macd_signal > 0) or (action == 2 and macd_signal < 0):
            reward += self.momentum_alignment_reward
        else:
            reward -= self.momentum_misalignment_penalty
    
    # 6. HOLDING PERIOD REWARDS
    
    # Reward patience (holding for reasonable periods)
    if self.position > 0 and action == 0:  # Holding position
        hold_duration = self.current_step - self.last_buy_step
        if self.min_hold_period <= hold_duration <= self.optimal_hold_period:
            reward += self.patience_reward
        elif hold_duration < self.min_hold_period:
            reward -= self.impatience_penalty
        elif hold_duration > self.max_hold_period:
            reward -= self.excessive_hold_penalty
    
    # 7. TRADE QUALITY REWARDS
    
    # Win rate bonus
    if len(self.closed_trades) > 5:
        recent_win_rate = self.calculate_recent_win_rate(window=10)
        if recent_win_rate > 0.6:
            reward += self.high_win_rate_bonus
        elif recent_win_rate < 0.3:
            reward -= self.low_win_rate_penalty
    
    # Profit factor reward
    if len(self.closed_trades) > 10:
        profit_factor = self.calculate_profit_factor()
        if profit_factor > 1.5:
            reward += self.high_profit_factor_bonus
        elif profit_factor < 0.8:
            reward -= self.low_profit_factor_penalty
    
    # 8. BEHAVIORAL CONSISTENCY REWARDS
    
    # Reward consistent strategy (not random actions)
    action_consistency = self.calculate_action_consistency(window=20)
    if action_consistency > self.consistency_threshold:
        reward += self.consistency_reward
    else:
        reward -= self.inconsistency_penalty
    
    # 9. MARKET CONDITION ADAPTATION
    
    # Volatility adaptation reward
    market_volatility = self.calculate_market_volatility(window=50)
    if market_volatility > self.high_volatility_threshold:
        # Reward more conservative behavior in volatile markets
        if action == 0:  # Holding cash in volatile times
            reward += self.volatility_adaptation_reward
        elif trade_executed:
            reward -= self.volatile_market_trade_penalty
    
    # 10. TRANSACTION COST OPTIMIZATION
    
    # Penalty for excessive transaction costs
    if trade_executed:
        transaction_cost_ratio = (current_price * self.transaction_fee) / self.total_value
        if transaction_cost_ratio > self.max_acceptable_transaction_cost:
            reward -= self.excessive_transaction_cost_penalty
    
    # 11. DIVERSIFICATION REWARDS (for multi-asset systems)
    
    # If trading multiple assets, reward diversification
    if hasattr(self, 'positions_by_asset'):
        concentration_risk = self.calculate_concentration_risk()
        if concentration_risk < self.max_concentration_risk:
            reward += self.diversification_reward
        else:
            reward -= self.concentration_penalty
    
    # 12. RISK-ADJUSTED RETURN REWARDS
    
    # Sharpe ratio improvement reward
    if len(self.returns_history) > 30:
        current_sharpe = self.calculate_rolling_sharpe(window=30)
        if current_sharpe > self.target_sharpe_ratio:
            reward += self.sharpe_improvement_bonus
    
    # 13. MARKET REGIME AWARENESS
    
    # Bull/Bear market adaptation
    market_regime = self.detect_market_regime()
    if market_regime == 'bull' and action == 1:  # Buy in bull market
        reward += self.regime_alignment_reward
    elif market_regime == 'bear' and action == 2:  # Sell in bear market
        reward += self.regime_alignment_reward
    elif market_regime == 'sideways' and action == 0:  # Hold in sideways
        reward += self.regime_alignment_reward
    
    # 14. PSYCHOLOGICAL BIAS PENALTIES
    
    # Prevent revenge trading (trading after losses)
    if self.recent_loss and trade_executed:
        reward -= self.revenge_trading_penalty
    
    # Prevent overconfidence after wins
    if self.recent_big_win and trade_executed and self.position_size_increase > 1.5:
        reward -= self.overconfidence_penalty
    
    # 15. CAPITAL PRESERVATION REWARDS
    
    # Reward capital preservation during bad periods
    if self.consecutive_losing_days > 3 and self.total_value >= self.capital_preservation_threshold:
        reward += self.capital_preservation_reward
    
    # 16. ADAPTIVE LEARNING REWARDS
    
    # Reward improving performance over time
    if len(self.performance_history) > 100:
        recent_performance = np.mean(self.performance_history[-30:])
        older_performance = np.mean(self.performance_history[-100:-70])
        if recent_performance > older_performance:
            reward += self.learning_improvement_bonus
    
    # Apply stop loss and take profit rewards/penalties
    if sl_triggered:
        reward -= self.stop_loss_penalty
    if tp_triggered:
        reward += self.take_profit_reward
    
    # Apply trading frequency penalties
    if self.daily_trades > self.max_daily_trades:
        excess_trades = self.daily_trades - self.max_daily_trades
        penalty = self.excessive_trade_base_penalty ** excess_trades
        reward -= penalty
    
    return reward

# Helper methods for the reward system
def calculate_price_trend(self, window=20):
    """Calculate price trend over specified window"""
    if self.current_step < window:
        return 0
    
    prices = self.df['Close'].iloc[self.current_step-window:self.current_step]
    return (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]

def calculate_recent_win_rate(self, window=10):
    """Calculate win rate for recent trades"""
    if len(self.closed_trades) < window:
        return 0.5
    
    recent_trades = self.closed_trades[-window:]
    wins = sum(1 for trade in recent_trades if trade['profit'] > 0)
    return wins / len(recent_trades)

def calculate_profit_factor(self):
    """Calculate profit factor (gross profit / gross loss)"""
    if not self.closed_trades:
        return 1.0
    
    gross_profit = sum(trade['profit'] for trade in self.closed_trades if trade['profit'] > 0)
    gross_loss = abs(sum(trade['profit'] for trade in self.closed_trades if trade['profit'] < 0))
    
    return gross_profit / gross_loss if gross_loss > 0 else float('inf')

def calculate_action_consistency(self, window=20):
    """Measure consistency of actions taken"""
    if len(self.action_history) < window:
        return 0.5
    
    recent_actions = self.action_history[-window:]
    # Calculate entropy of actions (lower entropy = more consistent)
    unique, counts = np.unique(recent_actions, return_counts=True)
    entropy = -sum((c/len(recent_actions)) * np.log2(c/len(recent_actions)) for c in counts if c > 0)
    
    # Convert to consistency score (0-1, higher is more consistent)
    max_entropy = np.log2(3)  # Max entropy for 3 actions
    return 1 - (entropy / max_entropy)

def detect_market_regime(self):
    """Detect current market regime (bull/bear/sideways)"""
    if self.current_step < 50:
        return 'sideways'
    
    # Simple regime detection based on moving averages
    short_ma = self.df['Close'].iloc[self.current_step-10:self.current_step].mean()
    long_ma = self.df['Close'].iloc[self.current_step-50:self.current_step].mean()
    
    if short_ma > long_ma * 1.02:
        return 'bull'
    elif short_ma < long_ma * 0.98:
        return 'bear'
    else:
        return 'sideways'

# Configuration parameters for the reward system
class RewardConfig:
    def __init__(self):
        # Portfolio performance
        self.portfolio_change_multiplier = 100
        
        # Risk management
        self.drawdown_penalty_multiplier = 50
        self.volatility_penalty = 20
        self.max_acceptable_volatility = 0.3
        
        # Position sizing
        self.position_sizing_reward = 0.1
        self.over_leverage_penalty = 1.0
        
        # Technical indicators
        self.rsi_optimal_reward = 0.3
        self.rsi_suboptimal_penalty = 0.2
        self.rsi_neutral_reward = 0.1
        self.momentum_alignment_reward = 0.2
        self.momentum_misalignment_penalty = 0.15
        
        # Timing and holding
        self.trend_following_reward = 0.25
        self.counter_trend_penalty = 0.3
        self.patience_reward = 0.05
        self.impatience_penalty = 0.1
        self.excessive_hold_penalty = 0.08
        
        # Trade quality
        self.high_win_rate_bonus = 0.5
        self.low_win_rate_penalty = 0.3
        self.high_profit_factor_bonus = 0.4
        self.low_profit_factor_penalty = 0.2
        
        # Behavioral
        self.consistency_reward = 0.1
        self.inconsistency_penalty = 0.05
        self.consistency_threshold = 0.6
        
        # Market adaptation
        self.volatility_adaptation_reward = 0.15
        self.volatile_market_trade_penalty = 0.1
        self.regime_alignment_reward = 0.2
        
        # Risk management thresholds
        self.min_hold_period = 3
        self.optimal_hold_period = 15
        self.max_hold_period = 50
        self.target_sharpe_ratio = 1.0
        
        # Psychological bias prevention
        self.revenge_trading_penalty = 0.5
        self.overconfidence_penalty = 0.3
        
        # Capital preservation
        self.capital_preservation_reward = 0.2
        self.capital_preservation_threshold = 0.95  # 95% of initial capital
        
        # Learning and adaptation
        self.learning_improvement_bonus = 0.3